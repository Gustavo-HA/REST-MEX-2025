{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5227eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 0. Librerías\n",
    "# =========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9276558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 1. Cargar y preparar datos\n",
    "# =========================================\n",
    "CSV_PATH = \"/home/cesar/corpus-sintetico.csv\"   # ajusta si lo cambias de lugar\n",
    "MAX_VOCAB = 60_000\n",
    "MAX_LEN   = 120\n",
    "\n",
    "df = pd.read_csv(CSV_PATH).dropna(subset=[\"Review\"])\n",
    "\n",
    "# ----- codificadores -----\n",
    "enc_pol  = LabelEncoder().fit(df[\"Polarity\"])\n",
    "enc_typ  = LabelEncoder().fit(df[\"Type\"])\n",
    "enc_town = LabelEncoder().fit(df[\"Town\"])\n",
    "\n",
    "y_pol   = tf.keras.utils.to_categorical(enc_pol.transform(df[\"Polarity\"]), 5)\n",
    "y_type  = tf.keras.utils.to_categorical(enc_typ.transform(df[\"Type\"]), 3)\n",
    "y_town  = tf.keras.utils.to_categorical(enc_town.transform(df[\"Town\"]), 40)\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(df)),\n",
    "    test_size=0.1, random_state=42,\n",
    "    stratify=enc_pol.transform(df[\"Polarity\"])\n",
    ")\n",
    "\n",
    "# ----- tokenización -----\n",
    "tok = Tokenizer(num_words=MAX_VOCAB, oov_token=\"[OOV]\")\n",
    "tok.fit_on_texts(df.loc[train_idx, \"Review\"])\n",
    "\n",
    "def to_seq(texts):\n",
    "    return pad_sequences(\n",
    "        tok.texts_to_sequences(texts),\n",
    "        maxlen=MAX_LEN,\n",
    "        padding=\"post\",      # ←--- relleno al final\n",
    "        truncating=\"post\"    #     y corte al final,\n",
    "    )\n",
    "\n",
    "X_train = to_seq(df.loc[train_idx, \"Review\"])\n",
    "X_val   = to_seq(df.loc[val_idx, \"Review\"])\n",
    "\n",
    "y_train = {\n",
    "    \"dense_pol\":  y_pol [train_idx],\n",
    "    \"dense_type\": y_type[train_idx],\n",
    "    \"dense_town\": y_town[train_idx]\n",
    "}\n",
    "y_val = {\n",
    "    \"dense_pol\":  y_pol [val_idx],\n",
    "    \"dense_type\": y_type[val_idx],\n",
    "    \"dense_town\": y_town[val_idx]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97f4dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 120)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 120, 300)             1800000   ['input_2[0][0]']             \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 120, 256)             439296    ['embedding_1[0][0]']         \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " self_attention (MultiHeadS  (None, None, 256)            196608    ['bidirectional_1[0][0]']     \n",
      " elfAttention)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 256)                  0         ['self_attention[0][0]']      \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d (Glob  (None, 256)                  0         ['self_attention[0][0]']      \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 512)                  0         ['global_average_pooling1d[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'global_max_pooling1d[0][0]']\n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 512)                  0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   32832     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 32)                   16416     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 128)                  65664     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 64)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 32)                   0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 128)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_pol (Dense)           (None, 5)                    325       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_type (Dense)          (None, 3)                    99        ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_town (Dense)          (None, 40)                   5160      ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18756400 (71.55 MB)\n",
      "Trainable params: 18756400 (71.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3. Modelo con Mecanismo de Atención + Bi-LSTM\n",
    "# =========================================\n",
    "EMB_DIM = 300\n",
    "LSTM_UNITS = 128\n",
    "ATTENTION_HEADS = 4\n",
    "\n",
    "# ---- Capa de Atención Personalizada ----\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, head_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.query = self.add_weight(\n",
    "            shape=(input_shape[-1], self.num_heads * self.head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            name='query'\n",
    "        )\n",
    "        self.key = self.add_weight(\n",
    "            shape=(input_shape[-1], self.num_heads * self.head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            name='key'\n",
    "        )\n",
    "        self.value = self.add_weight(\n",
    "            shape=(input_shape[-1], self.num_heads * self.head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            name='value'\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        # Proyecciones\n",
    "        Q = tf.matmul(inputs, self.query)\n",
    "        K = tf.matmul(inputs, self.key)\n",
    "        V = tf.matmul(inputs, self.value)\n",
    "        \n",
    "        # Reshape para múltiples cabezas\n",
    "        Q = tf.reshape(Q, [batch_size, -1, self.num_heads, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size, -1, self.num_heads, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size, -1, self.num_heads, self.head_dim])\n",
    "        \n",
    "        # Atención escalada\n",
    "        scores = tf.einsum('bqhd,bkhd->bhqk', Q, K) / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        \n",
    "        # Convertir el mask a float32 para evitar incompatibilidad de tipo\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, dtype=tf.float32)  # Convierte el mask a float32\n",
    "            scores += (mask[:, None, None, :] * -1e9)\n",
    "            \n",
    "        attn_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        output = tf.einsum('bhqk,bkhd->bqhd', attn_weights, V)\n",
    "        output = tf.reshape(output, [batch_size, -1, self.num_heads * self.head_dim])\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ---- Arquitectura Principal ----\n",
    "inputs = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=\"int32\")\n",
    "x = tf.keras.layers.Embedding(MAX_VOCAB, EMB_DIM, mask_zero=True)(inputs)\n",
    "\n",
    "# Bi-LSTM\n",
    "lstm_out = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=True)\n",
    ")(x)\n",
    "\n",
    "# Mecanismo de Atención\n",
    "attn_out = MultiHeadSelfAttention(\n",
    "    num_heads=ATTENTION_HEADS,\n",
    "    head_dim=64,\n",
    "    name=\"self_attention\"\n",
    ")(lstm_out)\n",
    "\n",
    "# Pooling Jerárquico\n",
    "avg_pool = tf.keras.layers.GlobalAveragePooling1D()(attn_out)\n",
    "max_pool = tf.keras.layers.GlobalMaxPooling1D()(attn_out)\n",
    "concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "x = tf.keras.layers.Dropout(0.4)(concat)\n",
    "\n",
    "# Capas Específicas por Tarea\n",
    "def build_task_branch(input_layer, units, num_classes, name):\n",
    "    branch = tf.keras.layers.Dense(units, activation='relu')(input_layer)\n",
    "    branch = tf.keras.layers.Dropout(0.2)(branch)\n",
    "    return tf.keras.layers.Dense(num_classes, activation='softmax', name=name)(branch)\n",
    "\n",
    "out_pol = build_task_branch(x, 64, 5, \"dense_pol\")\n",
    "out_type = build_task_branch(x, 32, 3, \"dense_type\")\n",
    "out_town = build_task_branch(x, 128, 40, \"dense_town\")\n",
    "\n",
    "model = tf.keras.Model(inputs, [out_pol, out_type, out_town])\n",
    "\n",
    "# Compilación\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "    loss={\n",
    "        \"dense_pol\": \"categorical_crossentropy\",\n",
    "        \"dense_type\": \"categorical_crossentropy\",\n",
    "        \"dense_town\": \"categorical_crossentropy\"\n",
    "    },\n",
    "    loss_weights={\"dense_pol\": 2.0, \"dense_type\": 1.0, \"dense_town\": 3.0},\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e6c1200",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_pol' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Asumir que MAX_LEN y MAX_VOCAB están definidos previamente\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Asumir que X_train, X_test, y_train_pol, y_train_type, y_train_town están listos para ser usados.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Dividir los datos en entrenamiento y validación\u001b[39;00m\n\u001b[1;32m     15\u001b[0m X_train, X_val, y_train_pol, y_val_pol, y_train_type, y_val_type, y_train_town, y_val_town \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m---> 16\u001b[0m     X_train, \u001b[43my_train_pol\u001b[49m, y_train_type, y_train_town, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Ajusta el tamaño del vocabulario y longitud máxima de la secuencia\u001b[39;00m\n\u001b[1;32m     20\u001b[0m MAX_LEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m  \u001b[38;5;66;03m# La longitud de las secuencias\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_pol' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Asumir que MAX_LEN y MAX_VOCAB están definidos previamente\n",
    "# Asumir que X_train, X_test, y_train_pol, y_train_type, y_train_town están listos para ser usados.\n",
    "\n",
    "# Ejemplo de datos (reemplázalo con tus propios datos)\n",
    "# X_train = np.array(...)  # Secuencias de texto preprocesadas\n",
    "# y_train_pol = np.array(...)  # Etiquetas de polaridad\n",
    "# y_train_type = np.array(...)  # Etiquetas de tipo\n",
    "# y_train_town = np.array(...)  # Etiquetas de locación\n",
    "\n",
    "# Dividir los datos en entrenamiento y validación\n",
    "X_train, X_val, y_train_pol, y_val_pol, y_train_type, y_val_type, y_train_town, y_val_town = train_test_split(\n",
    "    X_train, y_train_pol, y_train_type, y_train_town, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Ajusta el tamaño del vocabulario y longitud máxima de la secuencia\n",
    "MAX_LEN = 120  # La longitud de las secuencias\n",
    "MAX_VOCAB = 10000  # Tamaño del vocabulario\n",
    "\n",
    "# Definir el modelo nuevamente si no está en memoria\n",
    "model = tf.keras.Model(inputs, [out_pol, out_type, out_town])\n",
    "\n",
    "# Compilación del modelo (si no se ha hecho antes)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "    loss={\n",
    "        \"dense_pol\": \"categorical_crossentropy\",\n",
    "        \"dense_type\": \"categorical_crossentropy\",\n",
    "        \"dense_town\": \"categorical_crossentropy\"\n",
    "    },\n",
    "    loss_weights={\"dense_pol\": 2.0, \"dense_type\": 1.0, \"dense_town\": 3.0},\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    {\"dense_pol\": y_train_pol, \"dense_type\": y_train_type, \"dense_town\": y_train_town},  # Salidas\n",
    "    epochs=10,  # Número de épocas\n",
    "    batch_size=64,  # Tamaño del batch\n",
    "    validation_data=(X_val, {\"dense_pol\": y_val_pol, \"dense_type\": y_val_type, \"dense_town\": y_val_town}),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"modelo_atencion_bi_lstm.h5\")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de validación\n",
    "val_loss, val_accuracy = model.evaluate(\n",
    "    X_val, \n",
    "    {\"dense_pol\": y_val_pol, \"dense_type\": y_val_type, \"dense_town\": y_val_town},\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f'Pérdida en validación: {val_loss}')\n",
    "print(f'Precisión en validación: {val_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cebe969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 18:37:14.507308: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-07 18:37:14.507381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-07 18:37:14.509123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-07 18:37:14.517491: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-07 18:38:05.907665: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 18:38:06.019911: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 18:38:06.020421: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 18:38:06.022941: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 18:38:06.023437: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 18:38:06.023844: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 18:38:06.127952: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 18:38:06.128744: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 18:38:06.129113: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 18:38:06.129668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2788 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 120)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 120, 300)             1800000   ['input_1[0][0]']             \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 120, 256)             439296    ['embedding[0][0]']           \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " self_attention (MultiHeadS  (None, None, 256)            196608    ['bidirectional[0][0]']       \n",
      " elfAttention)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 256)                  0         ['self_attention[0][0]']      \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d (Glob  (None, 256)                  0         ['self_attention[0][0]']      \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 512)                  0         ['global_average_pooling1d[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'global_max_pooling1d[0][0]']\n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 512)                  0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   32832     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 32)                   16416     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 128)                  65664     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 64)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 32)                   0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 128)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_pol (Dense)           (None, 5)                    325       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_type (Dense)          (None, 3)                    99        ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_town (Dense)          (None, 40)                   5160      ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18756400 (71.55 MB)\n",
      "Trainable params: 18756400 (71.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 18:38:08.299374: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 262677600 exceeds 10% of free system memory.\n",
      "2025-05-07 18:38:08.617909: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 87559200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 18:38:08.752649: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 87559200 exceeds 10% of free system memory.\n",
      "2025-05-07 18:38:14.272183: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond_38/output/_24'\n",
      "2025-05-07 18:38:14.950853: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2025-05-07 18:38:15.854270: I external/local_xla/xla/service/service.cc:168] XLA service 0x79b9ec875010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-05-07 18:38:15.854307: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2025-05-07 18:38:15.882393: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746664696.047422    8861 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8551/8551 [==============================] - ETA: 0s - loss: 13.6290 - dense_pol_loss: 1.5934 - dense_type_loss: 1.0745 - dense_town_loss: 3.1226 - dense_pol_accuracy: 0.2428 - dense_type_accuracy: 0.4197 - dense_town_accuracy: 0.1927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 18:45:25.949534: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 29186880 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8551/8551 [==============================] - 452s 52ms/step - loss: 13.6290 - dense_pol_loss: 1.5934 - dense_type_loss: 1.0745 - dense_town_loss: 3.1226 - dense_pol_accuracy: 0.2428 - dense_type_accuracy: 0.4197 - dense_town_accuracy: 0.1927 - val_loss: 13.5231 - val_dense_pol_loss: 1.5835 - val_dense_type_loss: 1.0713 - val_dense_town_loss: 3.0949 - val_dense_pol_accuracy: 0.2478 - val_dense_type_accuracy: 0.4211 - val_dense_town_accuracy: 0.1960\n",
      "Epoch 2/10\n",
      "8551/8551 [==============================] - 379s 44ms/step - loss: 13.5029 - dense_pol_loss: 1.5833 - dense_type_loss: 1.0713 - dense_town_loss: 3.0884 - dense_pol_accuracy: 0.2476 - dense_type_accuracy: 0.4209 - dense_town_accuracy: 0.1973 - val_loss: 13.5009 - val_dense_pol_loss: 1.5801 - val_dense_type_loss: 1.0697 - val_dense_town_loss: 3.0903 - val_dense_pol_accuracy: 0.2492 - val_dense_type_accuracy: 0.4216 - val_dense_town_accuracy: 0.1976\n",
      "Epoch 3/10\n",
      "8551/8551 [==============================] - 376s 44ms/step - loss: 13.4842 - dense_pol_loss: 1.5806 - dense_type_loss: 1.0701 - dense_town_loss: 3.0843 - dense_pol_accuracy: 0.2489 - dense_type_accuracy: 0.4214 - dense_town_accuracy: 0.1984 - val_loss: 13.4941 - val_dense_pol_loss: 1.5789 - val_dense_type_loss: 1.0694 - val_dense_town_loss: 3.0890 - val_dense_pol_accuracy: 0.2498 - val_dense_type_accuracy: 0.4218 - val_dense_town_accuracy: 0.1980\n",
      "Epoch 4/10\n",
      "4090/8551 [=============>................] - ETA: 3:08 - loss: 13.4765 - dense_pol_loss: 1.5796 - dense_type_loss: 1.0696 - dense_town_loss: 3.0826 - dense_pol_accuracy: 0.2494 - dense_type_accuracy: 0.4222 - dense_town_accuracy: 0.1977"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 181\u001b[0m\n\u001b[1;32m    176\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# 3. Entrenamiento del modelo\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Salidas de entrenamiento\u001b[39;49;00m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Número de épocas\u001b[39;49;00m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tamaño del batch\u001b[39;49;00m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Conjunto de validación\u001b[39;49;00m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    188\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Guardar el modelo entrenado\u001b[39;00m\n\u001b[1;32m    191\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelo_atencion_bi_lstm.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/engine/training.py:1813\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1811\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1812\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1813\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/utils/generic_utils.py:296\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    293\u001b[0m         info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m info\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mio_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/utils/io_utils.py:81\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(message)\n\u001b[0;32m---> 81\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(message)\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/ipykernel/iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# =========================================\n",
    "# 1. Cargar y preparar datos\n",
    "# =========================================\n",
    "CSV_PATH = \"/home/cesar/Descargas/full_dataset_combined.csv\"   # ajusta si lo cambias de lugar\n",
    "MAX_VOCAB = 60_000\n",
    "MAX_LEN   = 120\n",
    "\n",
    "# df = (\n",
    "#    pd.read_csv(\"/home/cesar/Descargas/full_dataset_combined.csv\")\n",
    "#      .rename(columns={\"clean\": \"Review\"})\n",
    "#)\n",
    "\n",
    "# Cargar el dataframe\n",
    "df = pd.read_csv(CSV_PATH).dropna(subset=[\"clean\"]) # Review o clean\n",
    "\n",
    "# ----- codificadores -----\n",
    "enc_pol  = LabelEncoder().fit(df[\"Polarity\"])\n",
    "enc_typ  = LabelEncoder().fit(df[\"Type\"])\n",
    "enc_town = LabelEncoder().fit(df[\"Town\"])\n",
    "\n",
    "# Codificación de las etiquetas\n",
    "y_pol   = tf.keras.utils.to_categorical(enc_pol.transform(df[\"Polarity\"]), 5)\n",
    "y_type  = tf.keras.utils.to_categorical(enc_typ.transform(df[\"Type\"]), 3)\n",
    "y_town  = tf.keras.utils.to_categorical(enc_town.transform(df[\"Town\"]), 40)\n",
    "\n",
    "# División de datos en entrenamiento y validación\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(df)),\n",
    "    test_size=0.1, random_state=42,\n",
    "    stratify=enc_pol.transform(df[\"Polarity\"])\n",
    ")\n",
    "\n",
    "# ----- tokenización -----\n",
    "tok = Tokenizer(num_words=MAX_VOCAB, oov_token=\"[OOV]\")\n",
    "tok.fit_on_texts(df.loc[train_idx, \"clean\"])\n",
    "\n",
    "# Función para convertir textos a secuencias\n",
    "def to_seq(texts):\n",
    "    return pad_sequences(\n",
    "        tok.texts_to_sequences(texts),\n",
    "        maxlen=MAX_LEN,\n",
    "        padding=\"post\",      # ←--- relleno al final\n",
    "        truncating=\"post\"    #     y corte al final\n",
    "    )\n",
    "\n",
    "# Convertir los textos a secuencias\n",
    "X_train = to_seq(df.loc[train_idx, \"clean\"])\n",
    "X_val   = to_seq(df.loc[val_idx, \"clean\"])\n",
    "\n",
    "# Preparamos las etiquetas para el entrenamiento\n",
    "y_train = {\n",
    "    \"dense_pol\":  y_pol[train_idx],\n",
    "    \"dense_type\": y_type[train_idx],\n",
    "    \"dense_town\": y_town[train_idx]\n",
    "}\n",
    "\n",
    "y_val = {\n",
    "    \"dense_pol\":  y_pol[val_idx],\n",
    "    \"dense_type\": y_type[val_idx],\n",
    "    \"dense_town\": y_town[val_idx]\n",
    "}\n",
    "\n",
    "# =========================================\n",
    "# 2. Modelo con Mecanismo de Atención + Bi-LSTM\n",
    "# =========================================\n",
    "EMB_DIM = 300\n",
    "LSTM_UNITS = 128\n",
    "ATTENTION_HEADS = 4\n",
    "\n",
    "# ---- Capa de Atención Personalizada ----\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, head_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.query = self.add_weight(\n",
    "            shape=(input_shape[-1], self.num_heads * self.head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            name='query'\n",
    "        )\n",
    "        self.key = self.add_weight(\n",
    "            shape=(input_shape[-1], self.num_heads * self.head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            name='key'\n",
    "        )\n",
    "        self.value = self.add_weight(\n",
    "            shape=(input_shape[-1], self.num_heads * self.head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            name='value'\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        # Proyecciones\n",
    "        Q = tf.matmul(inputs, self.query)\n",
    "        K = tf.matmul(inputs, self.key)\n",
    "        V = tf.matmul(inputs, self.value)\n",
    "        \n",
    "        # Reshape para múltiples cabezas\n",
    "        Q = tf.reshape(Q, [batch_size, -1, self.num_heads, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size, -1, self.num_heads, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size, -1, self.num_heads, self.head_dim])\n",
    "        \n",
    "        # Atención escalada\n",
    "        scores = tf.einsum('bqhd,bkhd->bhqk', Q, K) / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        \n",
    "        # Convertir el mask a float32 para evitar incompatibilidad de tipo\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, dtype=tf.float32)  # Convierte el mask a float32\n",
    "            scores += (mask[:, None, None, :] * -1e9)\n",
    "            \n",
    "        attn_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        output = tf.einsum('bhqk,bkhd->bqhd', attn_weights, V)\n",
    "        output = tf.reshape(output, [batch_size, -1, self.num_heads * self.head_dim])\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ---- Arquitectura Principal ----\n",
    "inputs = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=\"int32\")\n",
    "x = tf.keras.layers.Embedding(MAX_VOCAB, EMB_DIM, mask_zero=True)(inputs)\n",
    "\n",
    "# Bi-LSTM\n",
    "lstm_out = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=True)\n",
    ")(x)\n",
    "\n",
    "# Mecanismo de Atención\n",
    "attn_out = MultiHeadSelfAttention(\n",
    "    num_heads=ATTENTION_HEADS,\n",
    "    head_dim=64,\n",
    "    name=\"self_attention\"\n",
    ")(lstm_out)\n",
    "\n",
    "# Pooling Jerárquico\n",
    "avg_pool = tf.keras.layers.GlobalAveragePooling1D()(attn_out)\n",
    "max_pool = tf.keras.layers.GlobalMaxPooling1D()(attn_out)\n",
    "concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "x = tf.keras.layers.Dropout(0.4)(concat)\n",
    "\n",
    "# Capas Específicas por Tarea\n",
    "def build_task_branch(input_layer, units, num_classes, name):\n",
    "    branch = tf.keras.layers.Dense(units, activation='relu')(input_layer)\n",
    "    branch = tf.keras.layers.Dropout(0.2)(branch)\n",
    "    return tf.keras.layers.Dense(num_classes, activation='softmax', name=name)(branch)\n",
    "\n",
    "out_pol = build_task_branch(x, 64, 5, \"dense_pol\")\n",
    "out_type = build_task_branch(x, 32, 3, \"dense_type\")\n",
    "out_town = build_task_branch(x, 128, 40, \"dense_town\")\n",
    "\n",
    "model = tf.keras.Model(inputs, [out_pol, out_type, out_town])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "    loss={\n",
    "        \"dense_pol\": \"categorical_crossentropy\",\n",
    "        \"dense_type\": \"categorical_crossentropy\",\n",
    "        \"dense_town\": \"categorical_crossentropy\"\n",
    "    },\n",
    "    loss_weights={\"dense_pol\": 2.0, \"dense_type\": 1.0, \"dense_town\": 3.0},\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# =========================================\n",
    "# 3. Entrenamiento del modelo\n",
    "# =========================================\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,  # Salidas de entrenamiento\n",
    "    epochs=10,  # Número de épocas\n",
    "    batch_size=64,  # Tamaño del batch\n",
    "    validation_data=(X_val, y_val),  # Conjunto de validación\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"modelo_atencion_bi_lstm.h5\")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de validación\n",
    "val_loss, val_accuracy = model.evaluate(\n",
    "    X_val, \n",
    "    y_val,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f'Pérdida en validación: {val_loss}')\n",
    "print(f'Precisión en validación: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1ca30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 8s 8ms/step\n",
      "F1 Polarity (Macro): 0.16363778986480731\n",
      "F1 Type (Macro): 0.21243740256950247\n",
      "F1 Town (Macro): 0.016172297907785307\n",
      "Macro F1: 0.130749163447365\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Realiza las predicciones del modelo en el conjunto de validación\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Para cada tarea, se realiza el cálculo del F1-score\n",
    "y_pred_pol = np.argmax(y_pred[0], axis=-1)  # Predicciones de polaridad\n",
    "y_pred_type = np.argmax(y_pred[1], axis=-1)  # Predicciones de tipo\n",
    "y_pred_town = np.argmax(y_pred[2], axis=-1)  # Predicciones de localidad\n",
    "\n",
    "# Las etiquetas verdaderas\n",
    "y_true_pol = np.argmax(y_val[\"dense_pol\"], axis=-1)\n",
    "y_true_type = np.argmax(y_val[\"dense_type\"], axis=-1)\n",
    "y_true_town = np.argmax(y_val[\"dense_town\"], axis=-1)\n",
    "\n",
    "# Cálculo del F1-score para cada tarea (Macro F1 por tarea)\n",
    "f1_pol = f1_score(y_true_pol, y_pred_pol, average='macro')\n",
    "f1_type = f1_score(y_true_type, y_pred_type, average='macro')\n",
    "f1_town = f1_score(y_true_town, y_pred_town, average='macro')\n",
    "\n",
    "# Cálculo del Macro F1 general (promedio de los tres F1s)\n",
    "macro_f1 = (f1_pol + f1_type + f1_town) / 3\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"F1 Polarity (Macro): {f1_pol}\")\n",
    "print(f\"F1 Type (Macro): {f1_type}\")\n",
    "print(f\"F1 Town (Macro): {f1_town}\")\n",
    "print(f\"Macro F1: {macro_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d14a5d3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Realiza las predicciones del modelo en el conjunto de validación\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Predicciones de polaridad (usando np.argmax para obtener la clase con mayor probabilidad)\n",
    "y_pred_pol = np.argmax(y_pred[0], axis=-1)\n",
    "\n",
    "# Etiquetas verdaderas de polaridad\n",
    "y_true_pol = np.argmax(y_val[\"dense_pol\"], axis=-1)\n",
    "\n",
    "# Genera la matriz de confusión para la polaridad\n",
    "conf_matrix = confusion_matrix(y_true_pol, y_pred_pol)\n",
    "\n",
    "# Crear un heatmap con seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=enc_pol.classes_, yticklabels=enc_pol.classes_)\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusión - Polaridad\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0752edad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51a0ae66f9a47c88c4f5a210410baba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb96f10e25de4176a19e07e38b1c857e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dae1e8776a452facb8ec519deb5121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e5d4e957fc480781bcf6c2a261c668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/480k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429e9aecf4574b24ab2bab2acba2061f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd8c20a2a3249d192b7a9701c7b5437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/dccuchile/bert-base-spanish-wwm-cased/e131a95091c777bbd45250fb647fec415010cb8cd1ad6e1d59babeb82a0be360?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1746644344&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NjY0NDM0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kY2N1Y2hpbGUvYmVydC1iYXNlLXNwYW5pc2gtd3dtLWNhc2VkL2UxMzFhOTUwOTFjNzc3YmJkNDUyNTBmYjY0N2ZlYzQxNTAxMGNiOGNkMWFkNmUxZDU5YmFiZWI4MmEwYmUzNjA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=UN5JYqio%7E3axV8VsrS4XYN9wfiM%7E-sexg4hImyuscKmkNFAWlvVClaCs%7EPwhnJVL%7ELGC-Efzsftqqkqleyy7AWBDzL8yOLbRcrvHKvOSzYx7sqrovjyezKQdxZjN2ixigGapSwDtR7KwYx90Qu7qjpturE3rO%7ElmP2KSjs6YPIp23kdQYvM7XjGe8XSHsJuL0GwY7MLPYpvkO7lLCdT7D1aWjOgzX62LhGVUjKuPNpDRQhZhvE-kVfjW5oeJh5bRkzw4lx0adUfXnjddjSg9eC5zOAraLc-1s2%7EEiCdkZnQ8%7E9NJQD3nO6HU0xPBOWItaaY-jIl9nOyvlRHDxMxx6A__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "dccuchile/bert-base-spanish-wwm-cased does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdccuchile/bert-base-spanish-wwm-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Nombre del modelo BETO\u001b[39;00m\n\u001b[1;32m     45\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ajusta el número de etiquetas según tus datos\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 3. Preprocesar los datos (tokenización y padding)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_texts\u001b[39m(texts):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Tokenizar las secuencias de texto\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/transformers/modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4251\u001b[0m     gguf_file\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4254\u001b[0m ):\n\u001b[1;32m   4255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4258\u001b[0m     )\n\u001b[0;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4278\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4279\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/transformers/modeling_utils.py:1100\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1095\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1096\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file without the variant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1097\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Use `variant=None` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m                 )\n\u001b[1;32m   1099\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1101\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m                 )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# to the original exception.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: dccuchile/bert-base-spanish-wwm-cased does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# =========================================\n",
    "# 1. Cargar y preparar datos\n",
    "# =========================================\n",
    "CSV_PATH = \"/home/cesar/corpus-sintetico.csv\"  # Ajusta el archivo CSV\n",
    "MAX_LEN = 120  # Longitud máxima de los textos\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Eliminar filas con valores nulos en la columna \"Review\"\n",
    "df = df.dropna(subset=[\"Review\"])\n",
    "\n",
    "# Asegurarse de que todos los valores en la columna 'Review' sean cadenas de texto\n",
    "df[\"Review\"] = df[\"Review\"].astype(str)\n",
    "\n",
    "# ----- codificadores -----\n",
    "enc_pol = LabelEncoder().fit(df[\"Polarity\"])\n",
    "enc_typ = LabelEncoder().fit(df[\"Type\"])\n",
    "enc_town = LabelEncoder().fit(df[\"Town\"])\n",
    "\n",
    "y_pol = enc_pol.transform(df[\"Polarity\"])\n",
    "y_type = enc_typ.transform(df[\"Type\"])\n",
    "y_town = enc_town.transform(df[\"Town\"])\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(df)),\n",
    "    test_size=0.1, random_state=42,\n",
    "    stratify=y_pol\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 2. Cargar el tokenizador y el modelo preentrenado BETO\n",
    "# =========================================\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"  # Nombre del modelo BETO\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=5)  # Ajusta el número de etiquetas según tus datos\n",
    "\n",
    "# =========================================\n",
    "# 3. Preprocesar los datos (tokenización y padding)\n",
    "# =========================================\n",
    "def encode_texts(texts):\n",
    "    # Tokenizar las secuencias de texto\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return encodings\n",
    "\n",
    "# Preprocesar las secuencias de entrenamiento y validación\n",
    "X_train_enc = encode_texts(df.loc[train_idx, \"Review\"])\n",
    "X_val_enc = encode_texts(df.loc[val_idx, \"Review\"])\n",
    "\n",
    "# Crear el dataset de PyTorch\n",
    "train_dataset = TensorDataset(\n",
    "    X_train_enc[\"input_ids\"].squeeze(),  # Eliminar las dimensiones extra\n",
    "    X_train_enc[\"attention_mask\"].squeeze(),\n",
    "    torch.tensor(y_pol[train_idx])\n",
    ")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    X_val_enc[\"input_ids\"].squeeze(),\n",
    "    X_val_enc[\"attention_mask\"].squeeze(),\n",
    "    torch.tensor(y_pol[val_idx])\n",
    ")\n",
    "\n",
    "# Crear los DataLoader para el entrenamiento y validación\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92c135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 12:28:07.296453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-07 12:28:07.296493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-07 12:28:07.320185: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-07 12:28:07.357784: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-07 12:28:19.422272: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 12:28:19.434971: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 12:28:19.435842: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 12:28:19.440511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 12:28:19.441597: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 12:28:19.442453: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 12:28:19.702360: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 12:28:19.703270: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 12:28:19.703647: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-07 12:28:19.703974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2788 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2025-05-07 12:28:22.049625: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 95238144 exceeds 10% of free system memory.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier', 'bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-07 12:30:15.707590: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 89798400 exceeds 10% of free system memory.\n",
      "2025-05-07 12:30:15.928938: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 89798400 exceeds 10% of free system memory.\n",
      "2025-05-07 12:30:16.123069: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 89798400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1209\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1208\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/voyager/lib/python3.10/site-packages/keras/src/engine/training.py:3983\u001b[0m, in \u001b[0;36mModel._assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_assert_compile_was_called\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3978\u001b[0m     \u001b[38;5;66;03m# Checks whether `compile` has been called. If it has been called,\u001b[39;00m\n\u001b[1;32m   3979\u001b[0m     \u001b[38;5;66;03m# then the optimizer is set. This is different from whether the\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m     \u001b[38;5;66;03m# model is compiled\u001b[39;00m\n\u001b[1;32m   3981\u001b[0m     \u001b[38;5;66;03m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[39;00m\n\u001b[1;32m   3982\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_compiled:\n\u001b[0;32m-> 3983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   3984\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must compile your model before \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3985\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining/testing. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3986\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.compile(optimizer, loss)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3987\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# =========================================\n",
    "# 1. Cargar y preparar datos\n",
    "# =========================================\n",
    "CSV_PATH = \"/home/cesar/corpus-sintetico.csv\"  # Ajusta el archivo CSV\n",
    "MAX_LEN = 120  # Longitud máxima de los textos\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Eliminar filas con valores nulos en la columna \"Review\"\n",
    "df = df.dropna(subset=[\"Review\"])\n",
    "\n",
    "# Asegurarse de que todos los valores en la columna 'Review' sean cadenas de texto\n",
    "df[\"Review\"] = df[\"Review\"].astype(str)\n",
    "\n",
    "# ----- codificadores -----\n",
    "enc_pol = LabelEncoder().fit(df[\"Polarity\"])\n",
    "enc_typ = LabelEncoder().fit(df[\"Type\"])\n",
    "enc_town = LabelEncoder().fit(df[\"Town\"])\n",
    "\n",
    "y_pol = enc_pol.transform(df[\"Polarity\"])\n",
    "y_type = enc_typ.transform(df[\"Type\"])\n",
    "y_town = enc_town.transform(df[\"Town\"])\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(df)),\n",
    "    test_size=0.1, random_state=42,\n",
    "    stratify=y_pol\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 2. Cargar el tokenizador y el modelo preentrenado BETO\n",
    "# =========================================\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"  # Nombre del modelo BETO\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "\n",
    "# =========================================\n",
    "# 3. Preprocesar los datos (tokenización y padding)\n",
    "# =========================================\n",
    "def encode_texts(texts):\n",
    "    # Asegurarse de que 'texts' sea una lista de cadenas\n",
    "    if isinstance(texts, pd.Series):  # Si es una Serie de pandas, convertirla a lista\n",
    "        texts = texts.tolist()\n",
    "    \n",
    "    # Tokenizar las secuencias de texto\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"tf\")\n",
    "    return encodings\n",
    "\n",
    "# Preprocesar las secuencias de entrenamiento y validación\n",
    "X_train_enc = encode_texts(df.loc[train_idx, \"Review\"])  # Convierte a lista si es necesario\n",
    "X_val_enc = encode_texts(df.loc[val_idx, \"Review\"])  # Convierte a lista si es necesario\n",
    "\n",
    "\n",
    "# Convertir a tensores de TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(X_train_enc),\n",
    "    tf.convert_to_tensor(y_pol[train_idx], dtype=tf.int32)\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(X_val_enc),\n",
    "    tf.convert_to_tensor(y_pol[val_idx], dtype=tf.int32)\n",
    "))\n",
    "\n",
    "# Configurar los datasets para la eficiencia\n",
    "train_dataset = train_dataset.batch(32).shuffle(100)\n",
    "val_dataset = val_dataset.batch(32)\n",
    "\n",
    "# =========================================\n",
    "# 4. Entrenamiento del modelo\n",
    "# =========================================\n",
    "optimizer = Adam(learning_rate=2e-5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=1)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da433b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voyager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
