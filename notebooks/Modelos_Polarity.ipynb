{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b715b6",
   "metadata": {},
   "source": [
    "## Entrenamiento y Evaluaci√≥n de Modelos BERT/BETO para An√°lisis de Sentimientos -variable 'Polarity' REST-MEX 2025\n",
    "\n",
    "Este notebook documenta el proceso completo de entrenamiento, validaci√≥n y evaluaci√≥n de modelos de lenguaje basados en BERT/BETO para la clasificaci√≥n de polaridad en textos del reto REST-MEX 2025. Incluye la preparaci√≥n y limpieza de datos, tokenizaci√≥n, manejo de desbalanceo de clases, definici√≥n de m√©tricas personalizadas, entrenamiento con GPU, generaci√≥n de reportes de desempe√±o (matriz de confusi√≥n, m√©tricas F1, precisi√≥n y recall por clase), as√≠ como la carga y evaluaci√≥n de checkpoints previos. El flujo permite experimentar con diferentes configuraciones y facilita la interpretaci√≥n de resultados para mejorar el desempe√±o del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a99a2a",
   "metadata": {},
   "source": [
    "### Lectura y limpieza de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b131802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np \n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Aseguramos que las stopwords est√©n disponibles\n",
    "try:\n",
    "    stopwords.words('spanish')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Ruta de lectura\n",
    "ruta = \"../data\"\n",
    "archivo = os.path.join(ruta, \"Rest-Mex_2025_train.csv\") \n",
    "\n",
    "with open(archivo, 'r', encoding='utf-8', errors='replace') as f:\n",
    "    Data = pd.read_csv(f)\n",
    "\n",
    "# Arreglamos problemas de codificaci√≥n\n",
    "def arregla_mojibake(texto):\n",
    "    try:\n",
    "        return texto.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return texto\n",
    "\n",
    "Data['Title'] = Data['Title'].fillna('').apply(arregla_mojibake)\n",
    "Data['Review'] = Data['Review'].fillna('').apply(arregla_mojibake)\n",
    "\n",
    "# Creamos columna base con texto le√≠do y concatenado\n",
    "Data['Texto_Leido'] = (Data['Title'] + ' ' + Data['Review']).str.strip()\n",
    "\n",
    "# Funci√≥n de limpieza profunda para modelos cl√°sicos\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    # Quitamos acentos\n",
    "    texto = unicodedata.normalize('NFD', texto)\n",
    "    texto = ''.join([char for char in texto if unicodedata.category(char) != 'Mn'])\n",
    "    # Eliminamos caracteres no alfab√©ticos\n",
    "    texto = re.sub(r'[^a-z√±√º\\s]', '', texto)\n",
    "    palabras = texto.split()\n",
    "    palabras = [p for p in palabras if p not in stopwords_es]\n",
    "    return ' '.join(palabras).strip()\n",
    "\n",
    "# Generamos columna limpia\n",
    "Data['Texto_Limpio'] = Data['Texto_Leido'].apply(limpiar_texto)\n",
    "\n",
    "# Eliminamos columnas originales\n",
    "Data = Data.drop(columns=['Title', 'Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La celda anterior se puede resumir en:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Ruta de lectura\n",
    "ruta = r\"C:\\Users\\uzgre\\Codes\\Python\\Ciencia de Datos\\Proyecto_final\\Rest-Mex_2025_DataSet\"\n",
    "archivo = os.path.join(ruta, \"Train_Limpio.csv\") \n",
    "\n",
    "with open(archivo, 'r', encoding='utf-8', errors='replace') as f:\n",
    "    Data = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281f8d3",
   "metadata": {},
   "source": [
    "### Verificamos CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df264ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True si est√° bien\n",
    "print(torch.cuda.get_device_name(0))  # Nombre de tu GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039d3cf",
   "metadata": {},
   "source": [
    "### Clasificacion con BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c53ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# 1. Cargar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "\n",
    "# 2. Asegurar que las etiquetas sean v√°lidas (1 a 5) y convertirlas a 0-4\n",
    "Data = Data[Data['Polarity'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "Data['label'] = Data['Polarity'].astype(int) - 1\n",
    "\n",
    "# 3. Split de datos\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    Data['Texto_Leido'].tolist(),\n",
    "    Data['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    stratify=Data['label']\n",
    ")\n",
    "\n",
    "# 4. Tokenizaci√≥n\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df0a63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Dataset personalizado\n",
    "class RMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RMDataset(train_encodings, train_labels)\n",
    "val_dataset = RMDataset(val_encodings, val_labels)\n",
    "\n",
    "# 6. C√°lculo de pesos de clase (para el dataset desbalanceado)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# 7. Cargar modelo BERT con clasificaci√≥n para 5 clases\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "    num_labels=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fe3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uzgre\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31209' max='31209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31209/31209 3:12:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.857200</td>\n",
       "      <td>1.037642</td>\n",
       "      <td>0.640456</td>\n",
       "      <td>0.664765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>1.019604</td>\n",
       "      <td>0.735527</td>\n",
       "      <td>0.742114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>1.102260</td>\n",
       "      <td>0.714931</td>\n",
       "      <td>0.729511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31209, training_loss=0.8307517970352004, metrics={'train_runtime': 11579.6044, 'train_samples_per_second': 43.121, 'train_steps_per_second': 2.695, 'total_flos': 3.284503772378112e+16, 'train_loss': 0.8307517970352004, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Definir funci√≥n de m√©tricas\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "''''\n",
    "from sklearn.metrics import accuracy_score, f1_score, \n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Accuracy y F1\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average='macro')\n",
    "    f1_micro = f1_score(labels, preds, average='micro')\n",
    "    f1_weighted = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "    }\n",
    "\n",
    "'''\n",
    "\n",
    "# 9. Usar Trainer personalizado para incluir class_weights\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# 10. Argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultados_beto\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# 11. Entrenador\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 12. ENTRENAMIENTO en GPU (si est√° disponible)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537cc9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 14. MATRIZ DE CONFUSI√ìN\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(val_labels, preds)\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     14\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m, xticklabels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m], yticklabels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 13. PREDICCIONES sobre el conjunto de validaci√≥n\n",
    "preds_output = trainer.predict(val_dataset)\n",
    "logits = preds_output.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "\n",
    "# 14. MATRIZ DE CONFUSI√ìN\n",
    "cm = confusion_matrix(val_labels, preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.xlabel(\"Etiqueta Predicha\")\n",
    "plt.ylabel(\"Etiqueta Verdadera\")\n",
    "plt.title(\"Matriz de Confusi√≥n BETO\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 15. REPORTE DETALLADO\n",
    "print(\"Reporte de Clasificaci√≥n (F1, Precision, Recall por clase):\")\n",
    "print(classification_report(val_labels, preds, digits=3, target_names=[\"Muy Neg\", \"Neg\", \"Neutro\", \"Pos\", \"Muy Pos\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7d043",
   "metadata": {},
   "source": [
    "### Reanudamos entrenamiento..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19060cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda560a99cf54ca687f804f7fe650b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5151cb3bde74a19aaf54c5d057adbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uzgre\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\uzgre\\AppData\\Local\\Temp\\ipykernel_12920\\507870675.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\uzgre\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "c:\\Users\\uzgre\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62418' max='62418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62418/62418 8:12:13, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.517889</td>\n",
       "      <td>0.778640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.544375</td>\n",
       "      <td>0.780995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.614318</td>\n",
       "      <td>0.777631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.765110</td>\n",
       "      <td>0.771479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=62418, training_loss=0.25837980316252807, metrics={'train_runtime': 29535.1911, 'train_samples_per_second': 33.812, 'train_steps_per_second': 2.113, 'total_flos': 6.569007544756224e+16, 'train_loss': 0.25837980316252807, 'epoch': 6.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Preparaci√≥n del Dataset\n",
    "Data['Polarity'] = Data['Polarity'].astype(int)\n",
    "Data = Data[Data['Polarity'].isin([1, 2, 3, 4, 5])]\n",
    "Data['labels'] = Data['Polarity'] - 1  # Etiquetas 0 a 4\n",
    "# Asegurar que todas las etiquetas est√©n entre 0 y 4\n",
    "assert Data['labels'].between(0, 4).all(), f\"Hay etiquetas fuera del rango 0-4: {Data['labels'].unique()}\"\n",
    "\n",
    "# Convertir a Dataset de Hugging Face\n",
    "dataset = Dataset.from_pandas(Data[['Texto_Leido', 'labels']])\n",
    "\n",
    "# 2. Tokenizador y modelo\n",
    "model_path = \"./resultados_beto/checkpoint-20806\"  # ‚Üê tu checkpoint previo\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# 3. Tokenizaci√≥n\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Texto_Leido\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Asegurar que las etiquetas est√©n en formato torch.long\n",
    "def cast_labels(batch):\n",
    "    batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(cast_labels)\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 4. Divisi√≥n en entrenamiento y validaci√≥n\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# 5. Argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultados_beto_mas_epocas\",  # nuevo directorio\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,  # ‚Üê solo guarda el checkpoint m√°s reciente\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_mas_epocas\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# 6. M√©tricas de evaluaci√≥n\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 8. Entrenamiento (¬°a dormir!)\n",
    "trainer.train(resume_from_checkpoint=model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77768b",
   "metadata": {},
   "source": [
    "### Cargamos modelos ya entrenados y hacemos predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto 1: MEXICANO Es un lugar bell√≠simo, para llegar es necesario transportarte en lancha la cual tiene un precio bastante accesible, en la isla venden varias artesan√≠as del lugar, desde prendas de vestir, antojitos mexicano, recuerditos y dem√°s, su gente es muy amable, la mayoria de las personas conservan su lengua nativa, el lugar es peque√±o pero f√°cil de caminar en una tarde, guarda muchas tradiciones mexicanas que se han perdido y te transporta a un lugar perdido en el tiempo\n",
      "‚Üí Polaridad real: 4.0  |  Predicci√≥n modelo: 5\n",
      "\n",
      "Texto 2: Una vista sensacional El lugar es muy agradable, con una vista realmente incre√≠ble, la atenci√≥n de los meseros muy buena, pero la comida no es nada fuera de lo com√∫n e incluso las porciones son peque√±as. La carta de vinos peque√±a, pero razonablemente completa.\n",
      "‚Üí Polaridad real: 3.0  |  Predicci√≥n modelo: 3\n",
      "\n",
      "Texto 3: Bastante recomendable para pasar la tarde Es una plaza en forma de herradura con muchos juegos infantiles para ni√±os, grandes jardines con pasto, varios restaurantes: comida mexicana, pollo rostizado, pizza gourmet, hamburguesas, cortes, micheladas, cafeteria y helados italianos (por cierto delicioso !!!, no te los puedes perder. Ojo que est√°n un poco apartados y cerca de un kiosco donde puedes ver a unos peces por lo que no te vayas a olvidar de ir). Tambi√©n hay un Bancomer con cajero por si necesitas efectivo. Tomate toda la tarde para ir, comer, sobremesa con caf√© o helado y tomar un par de cervezas. Amplio estacionamiento, gratis pero los locales venden la comida un poco cara para que lleves dinero ya que algunos solo manejan efectivo. Un lugar al que tienes que ir si vas a Ixtapan !!!\n",
      "‚Üí Polaridad real: 4.0  |  Predicci√≥n modelo: 4\n",
      "\n",
      "Texto 4: Dreams Tulum - algo para destacar Excelente atenci√≥n de Miguel Manzanero en el Seafood Restaurant de la playa durante el mediodia y de German en el Restaurant Asi√°tico por la noche. Victor del equipo de animaci√≥n un genio, gran jugador de volley y danzarin. Super cordiales y divertidos los 3.\n",
      "‚Üí Polaridad real: 4.0  |  Predicci√≥n modelo: 5\n",
      "\n",
      "Texto 5: Excelente servicio El desayuno esta s√∫per rico y el servicio fue bastante eficaz muy atentos desde el acceso y durante toda la atenci√≥n\n",
      "‚Üí Polaridad real: 5.0  |  Predicci√≥n modelo: 5\n",
      "\n",
      "Texto 6: gran desayuno el mejor plato de fruta en M√©xico; la terraza comedor fant√°sticas vistas; bicicletas para alquiler diario o semanal; excelente chillequilles y vegetales batidos\n",
      "‚Üí Polaridad real: 4.0  |  Predicci√≥n modelo: 4\n",
      "\n",
      "Texto 7: Inmejorable experiencia. Ampliamente recomendable. Entorno natural te hace sentir en el corazon de palenque ademas de mantener a los animales en su habiatat natural volveria con mi familia sin duda.\n",
      "‚Üí Polaridad real: 5.0  |  Predicci√≥n modelo: 5\n",
      "\n",
      "Texto 8: Muy interesante!!!! Muy bien preparado para el turista, la visita es muy did√°ctica. El lugar perfectamente impecable, el video, la atenci√≥n de su personal, vale la pena!\n",
      "‚Üí Polaridad real: 5.0  |  Predicci√≥n modelo: 5\n",
      "\n",
      "Texto 9: Lo pasamos genial en el Bahia Principe Tulum Dolphinarus Nos lo pasamos de maravilla nadar con los delfines en el Dolphinarus en el complejo Bah√≠a Pr√≠ncipe Tulum en cerca de Akumal. Los formadores son muy bueno y creo que mostraron que los delfines y los participantes un buen tiempo. Nos sentimos que aprendimos m√°s acerca de estas maravillosas criaturas y nos divertimos mucho. Tambi√©n nos alojamos por el espect√°culo de los delfines despu√©s.\n",
      "‚Üí Polaridad real: 5.0  |  Predicci√≥n modelo: 5\n",
      "\n",
      "Texto 10: Experiencia maravillosa Linda experiencia para comenzar la visita a Tulum. El personal es amable y la experiencia maravillosa. S√∫per recomendado\n",
      "‚Üí Polaridad real: 5.0  |  Predicci√≥n modelo: 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Seleccionar 3 textos al azar\n",
    "muestras = Data[['Texto_Leido', 'Polarity']].sample(10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "#checkpoint_path = \"./resultados_beto_mas_epocas/checkpoint-41612\"  # ‚Üê Ajusta con el n√∫mero real\n",
    "checkpoint_path = \"./resultados_beto/checkpoint-20806\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Enviar a dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Tokenizar entradas\n",
    "inputs = tokenizer(\n",
    "    list(muestras['Texto_Leido']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Predecir\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Mostrar resultados\n",
    "for i, row in muestras.iterrows():\n",
    "    print(f\"\\nTexto {i+1}: {row['Texto_Leido']}\")\n",
    "    print(f\"‚Üí Polaridad real: {row['Polarity']}  |  Predicci√≥n modelo: {preds[i]+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd71c13",
   "metadata": {},
   "source": [
    "### Resumen de metricas del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6541132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1.0, 'step': 10403, 'val_loss': 1.037642, 'accuracy': 0.640456, 'f1': 0.664765}\n",
      "{'epoch': 2.0, 'step': 20806, 'val_loss': 1.019604, 'accuracy': 0.735527, 'f1': 0.742114}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def mostrar_metricas_checkpoint(path):\n",
    "    with open(f\"{path}/trainer_state.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        state = json.load(f)\n",
    "\n",
    "    resumen = []\n",
    "    for log in state.get(\"log_history\", []):\n",
    "        if \"eval_loss\" in log:\n",
    "            resumen.append({\n",
    "                \"epoch\": log.get(\"epoch\"),\n",
    "                \"step\": log.get(\"step\"),\n",
    "                \"val_loss\": round(log.get(\"eval_loss\", 0), 6),\n",
    "                \"accuracy\": round(log.get(\"eval_accuracy\", 0), 6),\n",
    "                \"f1\": round(log.get(\"eval_f1\", 0), 6)  # ‚Üê solo si usaste F1 en compute_metrics\n",
    "            })\n",
    "    return resumen\n",
    "resumen = mostrar_metricas_checkpoint(\"./resultados_beto/checkpoint-20806\")\n",
    "for r in resumen:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b1f060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67032eda291442cc8bdd51053f913b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uzgre\\AppData\\Local\\Temp\\ipykernel_4952\\2242123223.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7715\n",
      "F1 Macro: 0.6953\n",
      "F1 Micro: 0.7715\n",
      "F1 Weighted: 0.7780\n",
      "\n",
      "Reporte completo:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7195    0.8654    0.7857      1070\n",
      "           1     0.6489    0.5452    0.5926      1139\n",
      "           2     0.7182    0.5850    0.6448      3072\n",
      "           3     0.5257    0.6743    0.5908      9073\n",
      "           4     0.8975    0.8306    0.8628     27257\n",
      "\n",
      "    accuracy                         0.7715     41611\n",
      "   macro avg     0.7020    0.7001    0.6953     41611\n",
      "weighted avg     0.7918    0.7715    0.7780     41611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Preparaci√≥n del Dataset\n",
    "Data['Polarity'] = Data['Polarity'].astype(int)\n",
    "Data = Data[Data['Polarity'].isin([1, 2, 3, 4, 5])]\n",
    "Data['labels'] = Data['Polarity'] - 1  # Etiquetas 0 a 4\n",
    "\n",
    "# Convertir a Dataset de Hugging Face\n",
    "dataset = Dataset.from_pandas(Data[['Texto_Leido', 'labels']])\n",
    "\n",
    "# 2. Tokenizaci√≥n\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Texto_Leido\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 3. Separar en evaluaci√≥n y entrenamiento\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# 4. Cargar modelo desde el checkpoint\n",
    "#model = BertForSequenceClassification.from_pretrained(\"./resultados_beto_mas_epocas/checkpoint-41612\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./resultados_beto/checkpoint-20806\")\n",
    "\n",
    "# 5. Crear Trainer solo para evaluar\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 6. Obtener predicciones\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# 7. Calcular m√©tricas\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "f1_weighted = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "print(f\"F1 Micro: {f1_micro:.4f}\")\n",
    "print(f\"F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(\"\\nReporte completo:\\n\")\n",
    "print(classification_report(labels, preds, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
