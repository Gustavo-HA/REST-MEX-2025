{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b715b6",
   "metadata": {},
   "source": [
    "## Entrenamiento y Evaluación de Modelos BERT/BETO para Análisis de Sentimientos -variable 'Polarity' REST-MEX 2025\n",
    "\n",
    "Este notebook documenta el proceso completo de entrenamiento, validación y evaluación de modelos de lenguaje basados en BERT/BETO para la clasificación de polaridad en textos del reto REST-MEX 2025. Incluye la preparación y limpieza de datos, tokenización, manejo de desbalanceo de clases, definición de métricas personalizadas, entrenamiento con GPU, generación de reportes de desempeño (matriz de confusión, métricas F1, precisión y recall por clase), así como la carga y evaluación de checkpoints previos. El flujo permite experimentar con diferentes configuraciones y facilita la interpretación de resultados para mejorar el desempeño del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a99a2a",
   "metadata": {},
   "source": [
    "### Lectura y limpieza de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b131802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np \n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Aseguramos que las stopwords estén disponibles\n",
    "try:\n",
    "    stopwords.words('spanish')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Ruta de lectura\n",
    "ruta = \"../data\"\n",
    "archivo = os.path.join(ruta, \"Rest-Mex_2025_train.csv\") \n",
    "\n",
    "with open(archivo, 'r', encoding='utf-8', errors='replace') as f:\n",
    "    Data = pd.read_csv(f)\n",
    "\n",
    "# Arreglamos problemas de codificación\n",
    "def arregla_mojibake(texto):\n",
    "    try:\n",
    "        return texto.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return texto\n",
    "\n",
    "Data['Title'] = Data['Title'].fillna('').apply(arregla_mojibake)\n",
    "Data['Review'] = Data['Review'].fillna('').apply(arregla_mojibake)\n",
    "\n",
    "# Creamos columna base con texto leído y concatenado\n",
    "Data['Texto_Leido'] = (Data['Title'] + ' ' + Data['Review']).str.strip()\n",
    "\n",
    "# Función de limpieza profunda para modelos clásicos\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    # Quitamos acentos\n",
    "    texto = unicodedata.normalize('NFD', texto)\n",
    "    texto = ''.join([char for char in texto if unicodedata.category(char) != 'Mn'])\n",
    "    # Eliminamos caracteres no alfabéticos\n",
    "    texto = re.sub(r'[^a-zñü\\s]', '', texto)\n",
    "    palabras = texto.split()\n",
    "    palabras = [p for p in palabras if p not in stopwords_es]\n",
    "    return ' '.join(palabras).strip()\n",
    "\n",
    "# Generamos columna limpia\n",
    "Data['Texto_Limpio'] = Data['Texto_Leido'].apply(limpiar_texto)\n",
    "\n",
    "# Eliminamos columnas originales\n",
    "Data = Data.drop(columns=['Title', 'Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La celda anterior se puede resumir en:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Ruta de lectura\n",
    "ruta = r\"C:\\Users\\uzgre\\Codes\\Python\\Ciencia de Datos\\Proyecto_final\\Rest-Mex_2025_DataSet\"\n",
    "archivo = os.path.join(ruta, \"Train_Limpio.csv\") \n",
    "\n",
    "with open(archivo, 'r', encoding='utf-8', errors='replace') as f:\n",
    "    Data = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281f8d3",
   "metadata": {},
   "source": [
    "### Verificamos CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df264ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True si está bien\n",
    "print(torch.cuda.get_device_name(0))  # Nombre de tu GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039d3cf",
   "metadata": {},
   "source": [
    "### Clasificacion con BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c53ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# 1. Cargar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "\n",
    "# 2. Asegurar que las etiquetas sean válidas (1 a 5) y convertirlas a 0-4\n",
    "Data = Data[Data['Polarity'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "Data['label'] = Data['Polarity'].astype(int) - 1\n",
    "\n",
    "# 3. Split de datos\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    Data['Texto_Leido'].tolist(),\n",
    "    Data['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    stratify=Data['label']\n",
    ")\n",
    "\n",
    "# 4. Tokenización\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df0a63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Dataset personalizado\n",
    "class RMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RMDataset(train_encodings, train_labels)\n",
    "val_dataset = RMDataset(val_encodings, val_labels)\n",
    "\n",
    "# 6. Cálculo de pesos de clase (para el dataset desbalanceado)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# 7. Cargar modelo BERT con clasificación para 5 clases\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "    num_labels=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fe3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uzgre\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31209' max='31209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31209/31209 3:12:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.857200</td>\n",
       "      <td>1.037642</td>\n",
       "      <td>0.640456</td>\n",
       "      <td>0.664765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>1.019604</td>\n",
       "      <td>0.735527</td>\n",
       "      <td>0.742114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>1.102260</td>\n",
       "      <td>0.714931</td>\n",
       "      <td>0.729511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31209, training_loss=0.8307517970352004, metrics={'train_runtime': 11579.6044, 'train_samples_per_second': 43.121, 'train_steps_per_second': 2.695, 'total_flos': 3.284503772378112e+16, 'train_loss': 0.8307517970352004, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Definir función de métricas\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "''''\n",
    "from sklearn.metrics import accuracy_score, f1_score, \n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Accuracy y F1\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average='macro')\n",
    "    f1_micro = f1_score(labels, preds, average='micro')\n",
    "    f1_weighted = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "    }\n",
    "\n",
    "'''\n",
    "\n",
    "# 9. Usar Trainer personalizado para incluir class_weights\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# 10. Argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultados_beto\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# 11. Entrenador\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 12. ENTRENAMIENTO en GPU (si está disponible)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537cc9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 14. MATRIZ DE CONFUSIÓN\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(val_labels, preds)\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     14\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m, xticklabels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m], yticklabels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 13. PREDICCIONES sobre el conjunto de validación\n",
    "preds_output = trainer.predict(val_dataset)\n",
    "logits = preds_output.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "\n",
    "# 14. MATRIZ DE CONFUSIÓN\n",
    "cm = confusion_matrix(val_labels, preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.xlabel(\"Etiqueta Predicha\")\n",
    "plt.ylabel(\"Etiqueta Verdadera\")\n",
    "plt.title(\"Matriz de Confusión BETO\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 15. REPORTE DETALLADO\n",
    "print(\"Reporte de Clasificación (F1, Precision, Recall por clase):\")\n",
    "print(classification_report(val_labels, preds, digits=3, target_names=[\"Muy Neg\", \"Neg\", \"Neutro\", \"Pos\", \"Muy Pos\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7d043",
   "metadata": {},
   "source": [
    "### Reanudamos entrenamiento..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19060cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda560a99cf54ca687f804f7fe650b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5151cb3bde74a19aaf54c5d057adbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uzgre\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\uzgre\\AppData\\Local\\Temp\\ipykernel_12920\\507870675.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\uzgre\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "c:\\Users\\uzgre\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62418' max='62418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62418/62418 8:12:13, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.517889</td>\n",
       "      <td>0.778640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.544375</td>\n",
       "      <td>0.780995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.614318</td>\n",
       "      <td>0.777631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.765110</td>\n",
       "      <td>0.771479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=62418, training_loss=0.25837980316252807, metrics={'train_runtime': 29535.1911, 'train_samples_per_second': 33.812, 'train_steps_per_second': 2.113, 'total_flos': 6.569007544756224e+16, 'train_loss': 0.25837980316252807, 'epoch': 6.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Preparación del Dataset\n",
    "Data['Polarity'] = Data['Polarity'].astype(int)\n",
    "Data = Data[Data['Polarity'].isin([1, 2, 3, 4, 5])]\n",
    "Data['labels'] = Data['Polarity'] - 1  # Etiquetas 0 a 4\n",
    "# Asegurar que todas las etiquetas estén entre 0 y 4\n",
    "assert Data['labels'].between(0, 4).all(), f\"Hay etiquetas fuera del rango 0-4: {Data['labels'].unique()}\"\n",
    "\n",
    "# Convertir a Dataset de Hugging Face\n",
    "dataset = Dataset.from_pandas(Data[['Texto_Leido', 'labels']])\n",
    "\n",
    "# 2. Tokenizador y modelo\n",
    "model_path = \"./resultados_beto/checkpoint-20806\"  # ← tu checkpoint previo\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# 3. Tokenización\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Texto_Leido\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Asegurar que las etiquetas estén en formato torch.long\n",
    "def cast_labels(batch):\n",
    "    batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(cast_labels)\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 4. División en entrenamiento y validación\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# 5. Argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultados_beto_mas_epocas\",  # nuevo directorio\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,  # ← solo guarda el checkpoint más reciente\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_mas_epocas\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# 6. Métricas de evaluación\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 8. Entrenamiento (¡a dormir!)\n",
    "trainer.train(resume_from_checkpoint=model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77768b",
   "metadata": {},
   "source": [
    "### Cargamos modelos ya entrenados y hacemos predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto 1: MEXICANO Es un lugar bellísimo, para llegar es necesario transportarte en lancha la cual tiene un precio bastante accesible, en la isla venden varias artesanías del lugar, desde prendas de vestir, antojitos mexicano, recuerditos y demás, su gente es muy amable, la mayoria de las personas conservan su lengua nativa, el lugar es pequeño pero fácil de caminar en una tarde, guarda muchas tradiciones mexicanas que se han perdido y te transporta a un lugar perdido en el tiempo\n",
      "→ Polaridad real: 4.0  |  Predicción modelo: 5\n",
      "\n",
      "Texto 2: Una vista sensacional El lugar es muy agradable, con una vista realmente increíble, la atención de los meseros muy buena, pero la comida no es nada fuera de lo común e incluso las porciones son pequeñas. La carta de vinos pequeña, pero razonablemente completa.\n",
      "→ Polaridad real: 3.0  |  Predicción modelo: 3\n",
      "\n",
      "Texto 3: Bastante recomendable para pasar la tarde Es una plaza en forma de herradura con muchos juegos infantiles para niños, grandes jardines con pasto, varios restaurantes: comida mexicana, pollo rostizado, pizza gourmet, hamburguesas, cortes, micheladas, cafeteria y helados italianos (por cierto delicioso !!!, no te los puedes perder. Ojo que están un poco apartados y cerca de un kiosco donde puedes ver a unos peces por lo que no te vayas a olvidar de ir). También hay un Bancomer con cajero por si necesitas efectivo. Tomate toda la tarde para ir, comer, sobremesa con café o helado y tomar un par de cervezas. Amplio estacionamiento, gratis pero los locales venden la comida un poco cara para que lleves dinero ya que algunos solo manejan efectivo. Un lugar al que tienes que ir si vas a Ixtapan !!!\n",
      "→ Polaridad real: 4.0  |  Predicción modelo: 4\n",
      "\n",
      "Texto 4: Dreams Tulum - algo para destacar Excelente atención de Miguel Manzanero en el Seafood Restaurant de la playa durante el mediodia y de German en el Restaurant Asiático por la noche. Victor del equipo de animación un genio, gran jugador de volley y danzarin. Super cordiales y divertidos los 3.\n",
      "→ Polaridad real: 4.0  |  Predicción modelo: 5\n",
      "\n",
      "Texto 5: Excelente servicio El desayuno esta súper rico y el servicio fue bastante eficaz muy atentos desde el acceso y durante toda la atención\n",
      "→ Polaridad real: 5.0  |  Predicción modelo: 5\n",
      "\n",
      "Texto 6: gran desayuno el mejor plato de fruta en México; la terraza comedor fantásticas vistas; bicicletas para alquiler diario o semanal; excelente chillequilles y vegetales batidos\n",
      "→ Polaridad real: 4.0  |  Predicción modelo: 4\n",
      "\n",
      "Texto 7: Inmejorable experiencia. Ampliamente recomendable. Entorno natural te hace sentir en el corazon de palenque ademas de mantener a los animales en su habiatat natural volveria con mi familia sin duda.\n",
      "→ Polaridad real: 5.0  |  Predicción modelo: 5\n",
      "\n",
      "Texto 8: Muy interesante!!!! Muy bien preparado para el turista, la visita es muy didáctica. El lugar perfectamente impecable, el video, la atención de su personal, vale la pena!\n",
      "→ Polaridad real: 5.0  |  Predicción modelo: 5\n",
      "\n",
      "Texto 9: Lo pasamos genial en el Bahia Principe Tulum Dolphinarus Nos lo pasamos de maravilla nadar con los delfines en el Dolphinarus en el complejo Bahía Príncipe Tulum en cerca de Akumal. Los formadores son muy bueno y creo que mostraron que los delfines y los participantes un buen tiempo. Nos sentimos que aprendimos más acerca de estas maravillosas criaturas y nos divertimos mucho. También nos alojamos por el espectáculo de los delfines después.\n",
      "→ Polaridad real: 5.0  |  Predicción modelo: 5\n",
      "\n",
      "Texto 10: Experiencia maravillosa Linda experiencia para comenzar la visita a Tulum. El personal es amable y la experiencia maravillosa. Súper recomendado\n",
      "→ Polaridad real: 5.0  |  Predicción modelo: 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Seleccionar 3 textos al azar\n",
    "muestras = Data[['Texto_Leido', 'Polarity']].sample(10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "#checkpoint_path = \"./resultados_beto_mas_epocas/checkpoint-41612\"  # ← Ajusta con el número real\n",
    "checkpoint_path = \"./resultados_beto/checkpoint-20806\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Enviar a dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Tokenizar entradas\n",
    "inputs = tokenizer(\n",
    "    list(muestras['Texto_Leido']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Predecir\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Mostrar resultados\n",
    "for i, row in muestras.iterrows():\n",
    "    print(f\"\\nTexto {i+1}: {row['Texto_Leido']}\")\n",
    "    print(f\"→ Polaridad real: {row['Polarity']}  |  Predicción modelo: {preds[i]+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd71c13",
   "metadata": {},
   "source": [
    "### Resumen de metricas del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6541132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1.0, 'step': 10403, 'val_loss': 1.037642, 'accuracy': 0.640456, 'f1': 0.664765}\n",
      "{'epoch': 2.0, 'step': 20806, 'val_loss': 1.019604, 'accuracy': 0.735527, 'f1': 0.742114}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def mostrar_metricas_checkpoint(path):\n",
    "    with open(f\"{path}/trainer_state.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        state = json.load(f)\n",
    "\n",
    "    resumen = []\n",
    "    for log in state.get(\"log_history\", []):\n",
    "        if \"eval_loss\" in log:\n",
    "            resumen.append({\n",
    "                \"epoch\": log.get(\"epoch\"),\n",
    "                \"step\": log.get(\"step\"),\n",
    "                \"val_loss\": round(log.get(\"eval_loss\", 0), 6),\n",
    "                \"accuracy\": round(log.get(\"eval_accuracy\", 0), 6),\n",
    "                \"f1\": round(log.get(\"eval_f1\", 0), 6)  # ← solo si usaste F1 en compute_metrics\n",
    "            })\n",
    "    return resumen\n",
    "resumen = mostrar_metricas_checkpoint(\"./resultados_beto/checkpoint-20806\")\n",
    "for r in resumen:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b1f060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67032eda291442cc8bdd51053f913b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uzgre\\AppData\\Local\\Temp\\ipykernel_4952\\2242123223.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7715\n",
      "F1 Macro: 0.6953\n",
      "F1 Micro: 0.7715\n",
      "F1 Weighted: 0.7780\n",
      "\n",
      "Reporte completo:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7195    0.8654    0.7857      1070\n",
      "           1     0.6489    0.5452    0.5926      1139\n",
      "           2     0.7182    0.5850    0.6448      3072\n",
      "           3     0.5257    0.6743    0.5908      9073\n",
      "           4     0.8975    0.8306    0.8628     27257\n",
      "\n",
      "    accuracy                         0.7715     41611\n",
      "   macro avg     0.7020    0.7001    0.6953     41611\n",
      "weighted avg     0.7918    0.7715    0.7780     41611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Preparación del Dataset\n",
    "Data['Polarity'] = Data['Polarity'].astype(int)\n",
    "Data = Data[Data['Polarity'].isin([1, 2, 3, 4, 5])]\n",
    "Data['labels'] = Data['Polarity'] - 1  # Etiquetas 0 a 4\n",
    "\n",
    "# Convertir a Dataset de Hugging Face\n",
    "dataset = Dataset.from_pandas(Data[['Texto_Leido', 'labels']])\n",
    "\n",
    "# 2. Tokenización\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Texto_Leido\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 3. Separar en evaluación y entrenamiento\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# 4. Cargar modelo desde el checkpoint\n",
    "#model = BertForSequenceClassification.from_pretrained(\"./resultados_beto_mas_epocas/checkpoint-41612\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./resultados_beto/checkpoint-20806\")\n",
    "\n",
    "# 5. Crear Trainer solo para evaluar\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 6. Obtener predicciones\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# 7. Calcular métricas\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "f1_weighted = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "print(f\"F1 Micro: {f1_micro:.4f}\")\n",
    "print(f\"F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(\"\\nReporte completo:\\n\")\n",
    "print(classification_report(labels, preds, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
