{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'c:\\\\Users\\\\Gus\\\\Documents\\\\proyectos\\\\REST-MEX-2025\\\\src\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import shutil\n",
    "\n",
    "from argparse import Namespace\n",
    "arg = Namespace()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if os.path.abspath(\"../src\") not in sys.path:\n",
    "    sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import preprocessing as pp\n",
    "import config\n",
    "import importlib\n",
    "\n",
    "importlib.reload(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Polarity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Town",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Region",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2887859b-55d3-4dc3-b60a-65d58c52a7b5",
       "rows": [
        [
         "0",
         "Mi Lugar Favorito!!!!",
         "Excelente lugar para comer y pasar una buena noche!!!\nEl servicio es de primera y la comida exquisita!!!",
         "5.0",
         "Sayulita",
         "Nayarit",
         "Restaurant"
        ],
        [
         "1",
         "lugares interesantes para visitar",
         "andar mucho, así que un poco difícil para personas con niños pequeños, pero con mucha historia en la zona, y la diversión de aprender un poco de todo, y explorar las ruinas. La playa también era bastante agradable!",
         "4.0",
         "Tulum",
         "QuintanaRoo",
         "Attractive"
        ],
        [
         "2",
         "No es el mismo Dreams ",
         "Es nuestra cuarta visita a Dreams Tulum, elegimos este hotel para festejar mi cumpleaños ya que en este hotel nos comprometimos y casamos y tenemos un cariño muy especial por este lugar, pero mostramos que cambiaron las cosas.  En cuestión de instalaciones sigue perfecto!! La playa muy limpia a pesar del sargazo ( es una cuestión natural incontrolable).   Pero en la amabilidad y servicio que los distinguía lo han perdido bastante, los empleados andan corriendo por todos lados, gritando de un lado a otro tratando de organizarse y pasamos varios detalles como por ejemplo mi esposo pidió un juego verde y la mesera le contestó que se parara él que estaba en la esquina porque solo se llevaba el café!! Eso jamás hubiera pasado en el Dreams de antes!!! Cuando uno se topaba al staff del",
         "3.0",
         "Tulum",
         "QuintanaRoo",
         "Hotel"
        ],
        [
         "3",
         "un buen panorama cerca de CancÃºn",
         "Estando en CancÃºn, fuimos al puerto y tomamos un Ferry a la Isla Mujeres.....despuÃ©s de un corto viaje, llegamos a esta pequeÃ±a isla, donde todo el mundo se desplaza en moto, carritos de golf, bicicleta o simplemente caminando.La recorrimos durante un rato y terminamos en la Playa Norte, donde pasamos la tarde recostadas sobre la arena y baÃ±Ã¡ndonos en el mar...... el agua tiene muy poca profundidad, por lo que puedes adentrarte mucho en el mar simplemente caminando.Si estÃ¡s en CancÃºn, te recomiendo destinar medio dÃ­a para conocer esta simpÃ¡tica isla.",
         "4.0",
         "Isla_Mujeres",
         "QuintanaRoo",
         "Attractive"
        ],
        [
         "4",
         "El mejor",
         "Es un lugar antiguo y por eso me encanto tiene un área de juegos gigante en la cual hay boliche, ping pong, mesas de cartas, dominó. Esta super céntrico Pase ahí año nuevo y la fiesta fue increíble También te prestan bicis para que visites la ciudad",
         "5.0",
         "Patzcuaro",
         "Michoacan",
         "Hotel"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Town</th>\n",
       "      <th>Region</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mi Lugar Favorito!!!!</td>\n",
       "      <td>Excelente lugar para comer y pasar una buena n...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Sayulita</td>\n",
       "      <td>Nayarit</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lugares interesantes para visitar</td>\n",
       "      <td>andar mucho, así que un poco difícil para pers...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Tulum</td>\n",
       "      <td>QuintanaRoo</td>\n",
       "      <td>Attractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No es el mismo Dreams</td>\n",
       "      <td>Es nuestra cuarta visita a Dreams Tulum, elegi...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Tulum</td>\n",
       "      <td>QuintanaRoo</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>un buen panorama cerca de CancÃºn</td>\n",
       "      <td>Estando en CancÃºn, fuimos al puerto y tomamos...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Isla_Mujeres</td>\n",
       "      <td>QuintanaRoo</td>\n",
       "      <td>Attractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El mejor</td>\n",
       "      <td>Es un lugar antiguo y por eso me encanto tiene...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Patzcuaro</td>\n",
       "      <td>Michoacan</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title  \\\n",
       "0              Mi Lugar Favorito!!!!   \n",
       "1  lugares interesantes para visitar   \n",
       "2             No es el mismo Dreams    \n",
       "3  un buen panorama cerca de CancÃºn   \n",
       "4                           El mejor   \n",
       "\n",
       "                                              Review  Polarity          Town  \\\n",
       "0  Excelente lugar para comer y pasar una buena n...       5.0      Sayulita   \n",
       "1  andar mucho, así que un poco difícil para pers...       4.0         Tulum   \n",
       "2  Es nuestra cuarta visita a Dreams Tulum, elegi...       3.0         Tulum   \n",
       "3  Estando en CancÃºn, fuimos al puerto y tomamos...       4.0  Isla_Mujeres   \n",
       "4  Es un lugar antiguo y por eso me encanto tiene...       5.0     Patzcuaro   \n",
       "\n",
       "        Region        Type  \n",
       "0      Nayarit  Restaurant  \n",
       "1  QuintanaRoo  Attractive  \n",
       "2  QuintanaRoo       Hotel  \n",
       "3  QuintanaRoo  Attractive  \n",
       "4    Michoacan       Hotel  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_csv(config.TRAIN_FILE)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set.drop(columns=config.TARGETS)\n",
    "y1 = train_set[config.TARGET1] # Polarity\n",
    "y2 = train_set[config.TARGET2] # Town\n",
    "y3 = train_set[config.TARGET3] # Type\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, y1, test_size=0.2, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X, y2, test_size=0.2, random_state=42)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X, y3, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproccess_pipeline():\n",
    "    \"\"\"\n",
    "    This function defines a preprocessing pipeline for text data.\n",
    "\n",
    "    Steps in the pipeline:\n",
    "    1. 'Arreglar mojibakes': Fixes mojibake issues in the specified text columns.\n",
    "    2. 'Minúsculas y quitar stopwords': Converts text to lowercase and removes stopwords from the specified text columns.\n",
    "    3. 'Quitar features no deseadas': Drops unwanted features from the specified text columns.\n",
    "\n",
    "    Args:\n",
    "        classifier: The classifier model to be used (not utilized in the pipeline itself).\n",
    "\n",
    "    Returns:\n",
    "        loan_pipe: A scikit-learn Pipeline object that applies the preprocessing steps sequentially.\n",
    "    \"\"\"\n",
    "    loan_pipe = Pipeline(\n",
    "    [\n",
    "        ('Arreglar mojibakes', pp.ArreglaMojibake(config.TEXT_COLUMNS)),\n",
    "        (\"Minúsculas y quitar stopwords\", pp.QuitaStopwords(config.TEXT_COLUMNS)),\n",
    "        (\"Guardar en una columna\", pp.JuntarFeatures(config.TEXT_COLUMNS, config.NEW_COLUMN)),\n",
    "        (\"Quitar features no deseadas\", pp.DropFeatures(config.TEXT_COLUMNS))\n",
    "    ]\n",
    "    )\n",
    "    return loan_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = preproccess_pipeline().fit_transform(X1_train)\n",
    "X2_train = preproccess_pipeline().fit_transform(X2_train)\n",
    "X3_train = preproccess_pipeline().fit_transform(X3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70541     Attractive\n",
       "186074    Attractive\n",
       "124863    Attractive\n",
       "79954          Hotel\n",
       "176039    Restaurant\n",
       "             ...    \n",
       "119879    Restaurant\n",
       "103694    Attractive\n",
       "131932    Attractive\n",
       "146867         Hotel\n",
       "121958    Restaurant\n",
       "Name: Type, Length: 166440, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoder\n",
    "le = LabelEncoder()\n",
    "y3_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abajo', 'abierta', 'abierto', ..., 'único', 'único inconveniente',\n",
       "       'únicos'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizer TFIDF\n",
    "arg.tfidf_max_features = 2500\n",
    "arg.tfidf_ngram_range = (1, 2)\n",
    "arg.token_pattern=r'(?u)\\b[^\\d\\W]+\\b'\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=arg.tfidf_max_features, \n",
    "                                   ngram_range=arg.tfidf_ngram_range, \n",
    "                                   token_pattern=arg.token_pattern)\n",
    "tfidf_vectorizer.fit(df_targetType[\"Texto_Limpio\"])\n",
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos en un Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class restaurantDataset(Dataset):\n",
    "    def __init__(self, data : pd.DataFrame, vectorizer, label_encoder,\n",
    "                 y : str, use_pca = False, pca = None, n_components = 3,\n",
    "                 standardize = False):\n",
    "        \"\"\"\n",
    "        Initializes the restaurantDataset class.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The input dataframe containing the text and labels.\n",
    "            vectorizer (TfidfVectorizer): The TF-IDF vectorizer to transform text data.\n",
    "            label_encoder (LabelEncoder): The label encoder to transform labels into numerical format.\n",
    "            y (str): The column name in the dataframe representing the target labels.\n",
    "            use_pca (bool, optional): Whether to apply PCA for dimensionality reduction. Default is False.\n",
    "            pca (PCA, optional): The PCA object to use for dimensionality reduction. Required if use_pca is True.\n",
    "            n_components (int, optional): The number of principal components to retain if PCA is applied. Default is 3.\n",
    "\n",
    "        Attributes:\n",
    "            data (pd.DataFrame): The input dataframe.\n",
    "            n_samples (int): The number of samples in the dataset.\n",
    "            X (torch.Tensor): The transformed feature matrix (TF-IDF or PCA-reduced).\n",
    "            y (torch.Tensor): The transformed target labels.\n",
    "            fitted_pca (PCA or None): The fitted PCA object if PCA is applied, otherwise None.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.n_samples = len(data)\n",
    "        \n",
    "        # Transform text to TF-IDF vectors\n",
    "        tfidf_matrix = vectorizer.transform(data[\"Texto_Limpio\"])\n",
    "\n",
    "        if use_pca:\n",
    "            pca.fit(tfidf_matrix.toarray())\n",
    "            self.X = torch.tensor(pca.transform(tfidf_matrix.toarray()), dtype=torch.float32)\n",
    "            self.fitted_pca = pca\n",
    "        else:\n",
    "            self.X = torch.tensor(tfidf_matrix.toarray(), dtype=torch.float32)\n",
    "            self.fitted_pca = None\n",
    "        \n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            self.X = torch.tensor(scaler.fit_transform(self.X), dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        # Transform labels to numbers\n",
    "        self.y = torch.tensor(label_encoder.transform(data[y]),\n",
    "                              dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg.use_pca = True\n",
    "arg.pca_n_components = 3\n",
    "arg.pca = PCA(n_components=arg.pca_n_components)\n",
    "arg.standardize = True\n",
    "arg.y = \"Type\"\n",
    "\n",
    "dataset = restaurantDataset(df_targetType, tfidf_vectorizer, le,\n",
    "                               use_pca=arg.use_pca, pca=arg.pca,\n",
    "                               n_components=arg.pca_n_components,\n",
    "                               standardize=arg.standardize,\n",
    "                               y=arg.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del train set 166440\n",
      "Longitud del test set 41611\n",
      "Shape of X [N, C]: torch.Size([128, 3]) torch.float32\n",
      "Shape of y: torch.Size([128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Separamos en conjuntos de entrenamiento y de prueba.\n",
    "arg.random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.X, dataset.y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=arg.random_state,\n",
    "                                                    stratify=dataset.y)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_data = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "print(\"Longitud del train set\", len(train_data))\n",
    "print(\"Longitud del test set\", len(test_data))\n",
    "\n",
    "# Dataloaders\n",
    "arg.batch_size = 128\n",
    "train_dataloader = DataLoader(train_data, batch_size=arg.batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=arg.batch_size)\n",
    "\n",
    "for i, (X, y) in enumerate(test_dataloader):\n",
    "    print(f\"Shape of X [N, C]: {X.shape} {X.dtype}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer modelo - Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "PrimerModelo(\n",
      "  (fc1): Linear(in_features=3, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=6, bias=True)\n",
      "  (fc3): Linear(in_features=6, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class PrimerModelo(nn.Module):\n",
    "    def __init__(self, input_size, output_size=3):\n",
    "        super(PrimerModelo, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 8)\n",
    "        self.fc2 = nn.Linear(8, 6)\n",
    "        self.fc3 = nn.Linear(6, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "model = PrimerModelo(input_size=3).to(device)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Entrena el modelo con los datos del dataloader y actualiza los pesos del modelo.\n",
    "    \n",
    "    Inputs:\n",
    "    - dataloader: DataLoader con los datos de entrenamiento.\n",
    "    - model: Modelo a entrenar.\n",
    "    - loss_fn: Función de pérdida.\n",
    "    - optimizer: Optimizador.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pth\",\n",
    "                    best_filename=\"model_best.pth\"):\n",
    "    \"\"\"\n",
    "    Save the model checkpoint to the specified path.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The state of the model to save, typically includes model weights and optimizer state.\n",
    "        is_best (bool): If True, saves a copy of the checkpoint as \"model_best.pth\".\n",
    "        checkpoint_path (str): The directory where the checkpoint will be saved.\n",
    "        filename (str): The name of the checkpoint file. Default is \"checkpoint.pth\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(checkpoint_path, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint_path, best_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gus\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "arg.lr = 2.3e-1\n",
    "arg.epochs = 100\n",
    "arg.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters\n",
    "arg.lr_patience = 10\n",
    "arg.lr_factor = 0.5 # Se reduce el learning rate a la mitad cada 10 epochs\n",
    "# sin mejorar el desempeño\n",
    "\n",
    "# Saving directory\n",
    "arg.savedir = \"../model\"\n",
    "os.makedirs(arg.savedir, exist_ok=True)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode = \"max\",\n",
    "    patience=arg.lr_patience,\n",
    "    factor=arg.lr_factor,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.099262  [  128/166440]\n",
      "loss: 0.397366  [12928/166440]\n",
      "loss: 0.335156  [25728/166440]\n",
      "loss: 0.309486  [38528/166440]\n",
      "loss: 0.328082  [51328/166440]\n",
      "loss: 0.407508  [64128/166440]\n",
      "loss: 0.437661  [76928/166440]\n",
      "loss: 0.358074  [89728/166440]\n",
      "loss: 0.344479  [102528/166440]\n",
      "loss: 0.373660  [115328/166440]\n",
      "loss: 0.351226  [128128/166440]\n",
      "loss: 0.395701  [140928/166440]\n",
      "loss: 0.343542  [153728/166440]\n",
      "loss: 0.240045  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.335335 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.398991  [  128/166440]\n",
      "loss: 0.388903  [12928/166440]\n",
      "loss: 0.329759  [25728/166440]\n",
      "loss: 0.301891  [38528/166440]\n",
      "loss: 0.313728  [51328/166440]\n",
      "loss: 0.411297  [64128/166440]\n",
      "loss: 0.437359  [76928/166440]\n",
      "loss: 0.361498  [89728/166440]\n",
      "loss: 0.347695  [102528/166440]\n",
      "loss: 0.374444  [115328/166440]\n",
      "loss: 0.343224  [128128/166440]\n",
      "loss: 0.400902  [140928/166440]\n",
      "loss: 0.347098  [153728/166440]\n",
      "loss: 0.244317  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334557 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.398001  [  128/166440]\n",
      "loss: 0.389587  [12928/166440]\n",
      "loss: 0.329622  [25728/166440]\n",
      "loss: 0.304064  [38528/166440]\n",
      "loss: 0.312008  [51328/166440]\n",
      "loss: 0.413549  [64128/166440]\n",
      "loss: 0.437723  [76928/166440]\n",
      "loss: 0.364498  [89728/166440]\n",
      "loss: 0.347716  [102528/166440]\n",
      "loss: 0.376653  [115328/166440]\n",
      "loss: 0.346068  [128128/166440]\n",
      "loss: 0.403402  [140928/166440]\n",
      "loss: 0.346644  [153728/166440]\n",
      "loss: 0.246946  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334330 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.396799  [  128/166440]\n",
      "loss: 0.390887  [12928/166440]\n",
      "loss: 0.330145  [25728/166440]\n",
      "loss: 0.303220  [38528/166440]\n",
      "loss: 0.314318  [51328/166440]\n",
      "loss: 0.419459  [64128/166440]\n",
      "loss: 0.437961  [76928/166440]\n",
      "loss: 0.362845  [89728/166440]\n",
      "loss: 0.351727  [102528/166440]\n",
      "loss: 0.376042  [115328/166440]\n",
      "loss: 0.345280  [128128/166440]\n",
      "loss: 0.403052  [140928/166440]\n",
      "loss: 0.346147  [153728/166440]\n",
      "loss: 0.244466  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334480 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.398794  [  128/166440]\n",
      "loss: 0.392079  [12928/166440]\n",
      "loss: 0.326779  [25728/166440]\n",
      "loss: 0.302279  [38528/166440]\n",
      "loss: 0.313112  [51328/166440]\n",
      "loss: 0.416557  [64128/166440]\n",
      "loss: 0.436808  [76928/166440]\n",
      "loss: 0.363232  [89728/166440]\n",
      "loss: 0.352665  [102528/166440]\n",
      "loss: 0.377533  [115328/166440]\n",
      "loss: 0.347635  [128128/166440]\n",
      "loss: 0.404886  [140928/166440]\n",
      "loss: 0.343524  [153728/166440]\n",
      "loss: 0.245549  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334935 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.398078  [  128/166440]\n",
      "loss: 0.391705  [12928/166440]\n",
      "loss: 0.332023  [25728/166440]\n",
      "loss: 0.300321  [38528/166440]\n",
      "loss: 0.316743  [51328/166440]\n",
      "loss: 0.415235  [64128/166440]\n",
      "loss: 0.437522  [76928/166440]\n",
      "loss: 0.363065  [89728/166440]\n",
      "loss: 0.352256  [102528/166440]\n",
      "loss: 0.377226  [115328/166440]\n",
      "loss: 0.347204  [128128/166440]\n",
      "loss: 0.405376  [140928/166440]\n",
      "loss: 0.342737  [153728/166440]\n",
      "loss: 0.247496  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334704 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.397894  [  128/166440]\n",
      "loss: 0.394241  [12928/166440]\n",
      "loss: 0.333593  [25728/166440]\n",
      "loss: 0.298483  [38528/166440]\n",
      "loss: 0.318516  [51328/166440]\n",
      "loss: 0.417637  [64128/166440]\n",
      "loss: 0.437944  [76928/166440]\n",
      "loss: 0.362538  [89728/166440]\n",
      "loss: 0.350937  [102528/166440]\n",
      "loss: 0.377341  [115328/166440]\n",
      "loss: 0.346022  [128128/166440]\n",
      "loss: 0.406211  [140928/166440]\n",
      "loss: 0.343959  [153728/166440]\n",
      "loss: 0.247842  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334373 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.395526  [  128/166440]\n",
      "loss: 0.394245  [12928/166440]\n",
      "loss: 0.334543  [25728/166440]\n",
      "loss: 0.298313  [38528/166440]\n",
      "loss: 0.317029  [51328/166440]\n",
      "loss: 0.417697  [64128/166440]\n",
      "loss: 0.438155  [76928/166440]\n",
      "loss: 0.361806  [89728/166440]\n",
      "loss: 0.348029  [102528/166440]\n",
      "loss: 0.377667  [115328/166440]\n",
      "loss: 0.346744  [128128/166440]\n",
      "loss: 0.407007  [140928/166440]\n",
      "loss: 0.342449  [153728/166440]\n",
      "loss: 0.243922  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334655 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.396949  [  128/166440]\n",
      "loss: 0.390592  [12928/166440]\n",
      "loss: 0.332850  [25728/166440]\n",
      "loss: 0.298878  [38528/166440]\n",
      "loss: 0.319110  [51328/166440]\n",
      "loss: 0.418171  [64128/166440]\n",
      "loss: 0.438749  [76928/166440]\n",
      "loss: 0.361527  [89728/166440]\n",
      "loss: 0.348902  [102528/166440]\n",
      "loss: 0.377138  [115328/166440]\n",
      "loss: 0.346885  [128128/166440]\n",
      "loss: 0.405852  [140928/166440]\n",
      "loss: 0.342341  [153728/166440]\n",
      "loss: 0.243548  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334699 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.396786  [  128/166440]\n",
      "loss: 0.389006  [12928/166440]\n",
      "loss: 0.332510  [25728/166440]\n",
      "loss: 0.298942  [38528/166440]\n",
      "loss: 0.320443  [51328/166440]\n",
      "loss: 0.417523  [64128/166440]\n",
      "loss: 0.439413  [76928/166440]\n",
      "loss: 0.361710  [89728/166440]\n",
      "loss: 0.348476  [102528/166440]\n",
      "loss: 0.378229  [115328/166440]\n",
      "loss: 0.346228  [128128/166440]\n",
      "loss: 0.405155  [140928/166440]\n",
      "loss: 0.342618  [153728/166440]\n",
      "loss: 0.244668  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334493 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.397050  [  128/166440]\n",
      "loss: 0.388873  [12928/166440]\n",
      "loss: 0.333245  [25728/166440]\n",
      "loss: 0.299428  [38528/166440]\n",
      "loss: 0.320536  [51328/166440]\n",
      "loss: 0.416929  [64128/166440]\n",
      "loss: 0.439790  [76928/166440]\n",
      "loss: 0.362662  [89728/166440]\n",
      "loss: 0.347421  [102528/166440]\n",
      "loss: 0.377208  [115328/166440]\n",
      "loss: 0.344937  [128128/166440]\n",
      "loss: 0.403809  [140928/166440]\n",
      "loss: 0.344001  [153728/166440]\n",
      "loss: 0.244092  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.334397 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.396996  [  128/166440]\n",
      "loss: 0.386941  [12928/166440]\n",
      "loss: 0.334752  [25728/166440]\n",
      "loss: 0.298380  [38528/166440]\n",
      "loss: 0.322193  [51328/166440]\n",
      "loss: 0.417352  [64128/166440]\n",
      "loss: 0.437605  [76928/166440]\n",
      "loss: 0.363210  [89728/166440]\n",
      "loss: 0.347805  [102528/166440]\n",
      "loss: 0.377175  [115328/166440]\n",
      "loss: 0.345690  [128128/166440]\n",
      "loss: 0.402348  [140928/166440]\n",
      "loss: 0.345361  [153728/166440]\n",
      "loss: 0.242220  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.334131 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.398096  [  128/166440]\n",
      "loss: 0.387082  [12928/166440]\n",
      "loss: 0.334225  [25728/166440]\n",
      "loss: 0.296509  [38528/166440]\n",
      "loss: 0.324402  [51328/166440]\n",
      "loss: 0.418396  [64128/166440]\n",
      "loss: 0.438043  [76928/166440]\n",
      "loss: 0.363451  [89728/166440]\n",
      "loss: 0.347020  [102528/166440]\n",
      "loss: 0.379526  [115328/166440]\n",
      "loss: 0.345705  [128128/166440]\n",
      "loss: 0.403030  [140928/166440]\n",
      "loss: 0.343914  [153728/166440]\n",
      "loss: 0.242161  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334065 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.398901  [  128/166440]\n",
      "loss: 0.386557  [12928/166440]\n",
      "loss: 0.334984  [25728/166440]\n",
      "loss: 0.296640  [38528/166440]\n",
      "loss: 0.324512  [51328/166440]\n",
      "loss: 0.417121  [64128/166440]\n",
      "loss: 0.440744  [76928/166440]\n",
      "loss: 0.363743  [89728/166440]\n",
      "loss: 0.346907  [102528/166440]\n",
      "loss: 0.383840  [115328/166440]\n",
      "loss: 0.348704  [128128/166440]\n",
      "loss: 0.401057  [140928/166440]\n",
      "loss: 0.344895  [153728/166440]\n",
      "loss: 0.244947  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334153 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.400612  [  128/166440]\n",
      "loss: 0.384255  [12928/166440]\n",
      "loss: 0.332882  [25728/166440]\n",
      "loss: 0.298749  [38528/166440]\n",
      "loss: 0.325547  [51328/166440]\n",
      "loss: 0.416071  [64128/166440]\n",
      "loss: 0.438397  [76928/166440]\n",
      "loss: 0.367627  [89728/166440]\n",
      "loss: 0.341827  [102528/166440]\n",
      "loss: 0.385649  [115328/166440]\n",
      "loss: 0.350063  [128128/166440]\n",
      "loss: 0.398168  [140928/166440]\n",
      "loss: 0.343037  [153728/166440]\n",
      "loss: 0.241941  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334162 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.404795  [  128/166440]\n",
      "loss: 0.386121  [12928/166440]\n",
      "loss: 0.334031  [25728/166440]\n",
      "loss: 0.297985  [38528/166440]\n",
      "loss: 0.325772  [51328/166440]\n",
      "loss: 0.412003  [64128/166440]\n",
      "loss: 0.437800  [76928/166440]\n",
      "loss: 0.370958  [89728/166440]\n",
      "loss: 0.340766  [102528/166440]\n",
      "loss: 0.382070  [115328/166440]\n",
      "loss: 0.350373  [128128/166440]\n",
      "loss: 0.398141  [140928/166440]\n",
      "loss: 0.342676  [153728/166440]\n",
      "loss: 0.241768  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.334321 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.407124  [  128/166440]\n",
      "loss: 0.400078  [12928/166440]\n",
      "loss: 0.333340  [25728/166440]\n",
      "loss: 0.294954  [38528/166440]\n",
      "loss: 0.322548  [51328/166440]\n",
      "loss: 0.412743  [64128/166440]\n",
      "loss: 0.433758  [76928/166440]\n",
      "loss: 0.362285  [89728/166440]\n",
      "loss: 0.332308  [102528/166440]\n",
      "loss: 0.374201  [115328/166440]\n",
      "loss: 0.349432  [128128/166440]\n",
      "loss: 0.392043  [140928/166440]\n",
      "loss: 0.339400  [153728/166440]\n",
      "loss: 0.237480  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.332434 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.405676  [  128/166440]\n",
      "loss: 0.400389  [12928/166440]\n",
      "loss: 0.333266  [25728/166440]\n",
      "loss: 0.294415  [38528/166440]\n",
      "loss: 0.322700  [51328/166440]\n",
      "loss: 0.411377  [64128/166440]\n",
      "loss: 0.433046  [76928/166440]\n",
      "loss: 0.362551  [89728/166440]\n",
      "loss: 0.333418  [102528/166440]\n",
      "loss: 0.373795  [115328/166440]\n",
      "loss: 0.349713  [128128/166440]\n",
      "loss: 0.392009  [140928/166440]\n",
      "loss: 0.338826  [153728/166440]\n",
      "loss: 0.238195  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.332325 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.406575  [  128/166440]\n",
      "loss: 0.401215  [12928/166440]\n",
      "loss: 0.332860  [25728/166440]\n",
      "loss: 0.295444  [38528/166440]\n",
      "loss: 0.321214  [51328/166440]\n",
      "loss: 0.412655  [64128/166440]\n",
      "loss: 0.432039  [76928/166440]\n",
      "loss: 0.362184  [89728/166440]\n",
      "loss: 0.333026  [102528/166440]\n",
      "loss: 0.373279  [115328/166440]\n",
      "loss: 0.347336  [128128/166440]\n",
      "loss: 0.391409  [140928/166440]\n",
      "loss: 0.338387  [153728/166440]\n",
      "loss: 0.238454  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.332231 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.407025  [  128/166440]\n",
      "loss: 0.403418  [12928/166440]\n",
      "loss: 0.333625  [25728/166440]\n",
      "loss: 0.295519  [38528/166440]\n",
      "loss: 0.320229  [51328/166440]\n",
      "loss: 0.411651  [64128/166440]\n",
      "loss: 0.432436  [76928/166440]\n",
      "loss: 0.362395  [89728/166440]\n",
      "loss: 0.332377  [102528/166440]\n",
      "loss: 0.374228  [115328/166440]\n",
      "loss: 0.347291  [128128/166440]\n",
      "loss: 0.390363  [140928/166440]\n",
      "loss: 0.337703  [153728/166440]\n",
      "loss: 0.236804  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.332029 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.407281  [  128/166440]\n",
      "loss: 0.405391  [12928/166440]\n",
      "loss: 0.333034  [25728/166440]\n",
      "loss: 0.295893  [38528/166440]\n",
      "loss: 0.318755  [51328/166440]\n",
      "loss: 0.411451  [64128/166440]\n",
      "loss: 0.432370  [76928/166440]\n",
      "loss: 0.361485  [89728/166440]\n",
      "loss: 0.332243  [102528/166440]\n",
      "loss: 0.374272  [115328/166440]\n",
      "loss: 0.347290  [128128/166440]\n",
      "loss: 0.389847  [140928/166440]\n",
      "loss: 0.337570  [153728/166440]\n",
      "loss: 0.235988  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.332000 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.406485  [  128/166440]\n",
      "loss: 0.405239  [12928/166440]\n",
      "loss: 0.332407  [25728/166440]\n",
      "loss: 0.295831  [38528/166440]\n",
      "loss: 0.319116  [51328/166440]\n",
      "loss: 0.410233  [64128/166440]\n",
      "loss: 0.432990  [76928/166440]\n",
      "loss: 0.361861  [89728/166440]\n",
      "loss: 0.331691  [102528/166440]\n",
      "loss: 0.375037  [115328/166440]\n",
      "loss: 0.347826  [128128/166440]\n",
      "loss: 0.389141  [140928/166440]\n",
      "loss: 0.337607  [153728/166440]\n",
      "loss: 0.235657  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.332064 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.406717  [  128/166440]\n",
      "loss: 0.405841  [12928/166440]\n",
      "loss: 0.331933  [25728/166440]\n",
      "loss: 0.295805  [38528/166440]\n",
      "loss: 0.320137  [51328/166440]\n",
      "loss: 0.411121  [64128/166440]\n",
      "loss: 0.432725  [76928/166440]\n",
      "loss: 0.362627  [89728/166440]\n",
      "loss: 0.331081  [102528/166440]\n",
      "loss: 0.375204  [115328/166440]\n",
      "loss: 0.347049  [128128/166440]\n",
      "loss: 0.389702  [140928/166440]\n",
      "loss: 0.338022  [153728/166440]\n",
      "loss: 0.234477  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.331994 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.406223  [  128/166440]\n",
      "loss: 0.405889  [12928/166440]\n",
      "loss: 0.332011  [25728/166440]\n",
      "loss: 0.295741  [38528/166440]\n",
      "loss: 0.320108  [51328/166440]\n",
      "loss: 0.411611  [64128/166440]\n",
      "loss: 0.432793  [76928/166440]\n",
      "loss: 0.363577  [89728/166440]\n",
      "loss: 0.330704  [102528/166440]\n",
      "loss: 0.375581  [115328/166440]\n",
      "loss: 0.346412  [128128/166440]\n",
      "loss: 0.388130  [140928/166440]\n",
      "loss: 0.337316  [153728/166440]\n",
      "loss: 0.233129  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.331934 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.405635  [  128/166440]\n",
      "loss: 0.405687  [12928/166440]\n",
      "loss: 0.330191  [25728/166440]\n",
      "loss: 0.296741  [38528/166440]\n",
      "loss: 0.319948  [51328/166440]\n",
      "loss: 0.410850  [64128/166440]\n",
      "loss: 0.433028  [76928/166440]\n",
      "loss: 0.363234  [89728/166440]\n",
      "loss: 0.331652  [102528/166440]\n",
      "loss: 0.374928  [115328/166440]\n",
      "loss: 0.345810  [128128/166440]\n",
      "loss: 0.388036  [140928/166440]\n",
      "loss: 0.337473  [153728/166440]\n",
      "loss: 0.234624  [52040/166440]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.331819 \n",
      "\n",
      "No improvement. Breaking out of loop.\n",
      "Ya quedó.\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "best_metric = 0\n",
    "n_no_improve = 0\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, criterion, optimizer)\n",
    "    tuning_metric = test(test_dataloader, model, criterion)\n",
    "    \n",
    "    # Update the scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    is_best = tuning_metric > best_metric\n",
    "    if is_best:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "    \n",
    "    save_checkpoint({\n",
    "        \"epoch\": t + 1,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"best_metric\": best_metric,\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }, is_best, arg.savedir, \n",
    "                    filename=\"checkpoint_primerModeloGus_std.pth\", \n",
    "                    best_filename=\"model-best_primerModeloGus_std.pth\")\n",
    "    \n",
    "    if n_no_improve >= arg.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        break\n",
    "    \n",
    "print(\"Ya quedó.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
